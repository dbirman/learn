<!DOCTYPE HTML>
<!--
	Landed by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<!-- // The code in this client is based on the code of Robert Hawkins: https://github.com/hawkrobe/MWERT
 -->
<html>
	<head>
		<title>Reinforcement learning lab</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!-- [if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif] -->
		<link rel="stylesheet" href="../assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		
		<!-- Generics -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/math.min.js"></script>
			<script src="../assets/js/jquery.csv.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/jquery.dropotron.min.js"></script>
			<script src="../assets/js/jquery.scrollex.min.js"></script>
			<script src="../assets/js/skel.min.js"></script>
			<script src="../assets/js/util.js"></script>			

			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../assets/js/main.js"></script>
		<!-- Math KaTeX -->
			<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">
			<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
		<!-- Plotting: Plot.ly JS -->
			<script type="text/javascript" src="../assets/js/mathjax/MathJax.js?config=TeX-AMS-MML_SVG"></script>
      <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
	</head>
	<body>

	<img id="dpsquirrel" src="images/deadpool.png" style="display:none;z-index:1;position:absolute;"/>
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">

					<h1 id="logo"><a href="../learn.html">Reinforcement learning lab</a></h1>
					<nav id="nav">
						<ul>
							<li><a href="../learn.html">Learn</a></li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<div id="main" class="wrapper style1">
					<div class="container">
						<header class="major">
							<h2>Reinforcement learning lab</h2>
							<p>PSYCH 50</p>
						</header>

						<!-- Content -->
							<section id="content">

<div id="block1">
	<h2>Reinforcement learning lab</h2>
	<p>This final lab is focused on helping you understand the reinforcement learning models we use in cognitive neuroscience. We want you to both realize their usefuleness but also their inherent limitations. Learning is complex--and reinforcement learning is able to capture many forms of learning. That said, there are many caveats, some of which we will discuss today. There is now an entire field dedicated to reinforcement learning but we're going to focus narrowly on just one specific idea: that you can estimate the <b>value</b> of an action by comparing your prediction against reality over and over.</p>
	<h1>Learning goals</h1>
	After this lab you should have a better understanding of the following:
	<ul>
		<li>How reinforcement learning and decision making are used together for action selection</li>
		<li>How the rescorla-wagner and softmax algorithms provide an algorithmic solution for reinforcement learning and decision making</li>
		<li>When these simple algorithms are effective and when they fail</li>
	</ul>
</div>

<div id="block2">
	<h2>Demo</h2>
	<p>The first part of this tutorial is a live demonstration of reinforcement learning in action. You will take on the role of a squirrel, your goal is to fatten yourself up to survive the Palo Alto winter. We all know how bad it can get, so collect a lot of apples! There will be three trees, each of which drops apples. Click on a tree to visit it--every few seconds apples drop from the tree you are visiting. You'll have to search among all of the trees to find the best tree... good luck!</p>
	<p>Head over to: <a href="http://gru.stanford.edu:8080" target="_blank">our local forest</a> to get started on the demo. Your TA will tell you which section to join.</p>
</div>

<div id="block3">
	<h2>Verbal model</h2>
	<p>Along with your class you should have had a discussion about a verbal model of learning in the apple hoarding task. Your model should have ended up being something like: "I keep an estimate of how many apples each tree drops in my mind. I pick a tree to go to and compare how many apples I get against what I expected. I adjust up or down accordingly". This is the verbal model that we're now going to translate into a computational one.</p>
	<p>We've done this before so we're now going to dig into more of the details than we have in past computational models. The first thing to do here is to take the computational goal we've described (the verbal model) and turn it into a mathematical equation. This is important: our goal as cognitive neuroscientists is to connect the dots between behavior and the brain. Formalizing learning will allow us to make specific predictions, both about what should be represented by the brain but also about future behaviors. These predictions are the key to testing whether reinforcement learning is how a squirrel might solve this particular task.</p>
	<p>The first part of our verbal model was: <b>keep an estimate of how many apples each tree drops</b>. For now we're going to model a world in which you make decisions about only one tree. What this means is that there's some variable that is going to keep track of our <b>knowledge</b> for a particular tree. We'll call this variable A, for apples:</p>
	<div id="katex1"></div>
	<p>A should probably start at our best guess about trees (before we get any information), which you could call your prior knowledge. We'll start at zero for simplicity, which sort of implies we're pessimistic. The second part of the verbal model says, "I pick a tree and compare how many apples I get against what I expected". Let's call the number of apples you get on each visit R, for <b>reward</b>. We'll call the difference between your knowledge and the reward the <b>reward prediction error</b>:</p>
	<div id="katex2"></div>
	<p>Notice that we indexed A with a little <b>v</b>? That's because we're now talking about a specific visit. Your knowledge about how many apples a tree drops is going to change over time so we need to be specific about when exactly we're talking about. Before we go on let's think about the reward prediction error for a bit.</p>
</div>

<div id="block4">
	<h2>Reward prediction error</h2>
	<p>To help you get a sense for how prediction error is useful let's imagine a world with a single tree. On each step we're going to let you make a guess of how many apples will fall and then check your answer. We won't tell you how many apples actually fell, we're just going to tell you whether the <b>prediction error</b> was positive (more apples fell than you expected) or negative. The point of this demo is to reinforce the idea that you don't need absolute knowledge to do well at estimating something's value, as long as you can integrate information over time. This should remind you of a similar story we heard about LIP neurons and MT! (The number of apples dropping is random... just for visualizing what's going on)</p>
	<canvas id="canvas4" height=400 width=400></canvas>
	<br>
	<input id="guess4v" type="range" style="width:300px;" name="Guess" min="0" max="10" step="0.1" value="0" oninput="out4(this.value);"><span id="guess4out">Guess: 0</span>
	<a class="button" id="guess4" onclick="run4();">Visit tree</a><br>
	<span id="out4"></span>
	<div id="end4">
	<br>
	<p>Nice work! Hopefully the reward prediction error makes sense. We're going to put everything together now and look at how the Rescorla-Wagner algorithm works.</p>
	</div>
</div>

<div id="block5">
	<h2>Rescorla-Wagner</h2>
	<p>The last thing we need to think about is the <b>learning rate</b>. The intuition we want you to have for the learning rate is that it indexes how much you trust the reward prediction error. If there is a lot of variability in the world then you shouldn't trust it too much, you want a low learning rate to give yourself time to build up a good representation of the world. If the world is very stable and there isn't any noise then you can trust the reward prediction error. Don't worry if that intuition doesn't make perfect sense, we're going to play with it below.</p>
	<p>Adding the learning rate, the complete model that we've now built up looks as follows:</p>
	<div id="katex3"></div>
	<p>In english: on each visit to the tree (currently <b>v</b>, the next visit is <b>v+1</b>) we take our current prediction, <b>Av</b>, and add to it the reward prediction error, reduced by the learning rate (the inverse of how noisy we think the environment is).</p>
	<p>Check out the graph below. It shows you the reward prediction error on each visit and the current estimate of the tree's value. We want you to get a sense for how alpha changes the learning over time--try tweaking alpha and then take a look at the examples we've put before.</p>
	<input type="range" style="width:200px;" min="0.01" max="1.00" step="0.01" value="0.50" oninput="learningrate5(this.value);"><span id="alpha5">Learning rate = 0.50</span><br>
	<div id="plot5"></div>
	<br>
	<p>To help orient you to the graph go through these activities and make sure you understand each question before continuing:</p>
	<ul>
		<li>Try setting the learning rate to a very small value. What verbal model would this translate to? What would your decisions at each time point rely on?</li>
		<li>What if the learning rate is one?</li>
		<li>Why does the RPE decrease, whereas the Av value increases and then stabilizes?</li>
	</ul>
</div>

<div id="block6">
	<h2>Multiple trees</h2>
	<p>In the demo you kept in mind the values of three trees. We can do this just fine with Rescorla-Wagner. Each tree simply gets updated when it gets visited, and otherwise the value doesn't change. But there's a different problem with having multiple problems which goes beyond reinforcement learning. Reinforcement learning is about discovering the <b>value</b> of taking different actions in the world. But given different choices with known values, how should you pick between them? In other words you were doing two things during the task: determining values and then using those values to make decisions. This next section will be focused on <b>decision making</b>.</p>
	<p>For today we want to contrast two ways of picking which tree to visit. Imagine that you believe that trees A, B, and C drop 1, 2, and 3 apples on average. Or as variables:</p>
	<div id="katex6"></div>
	<p>One rule you could use to decide where to go next is winner-take-all. Since tree C is worth the most you would go there. There are two possible outcomes for the <i>following</i> visit. If on this visit your belief about tree C improves or stays the same, you'll keep repeating this action. But if you were to get far less apples than expected, you might reduce your estimate of tree C's value enough to make tree B the best choice. In a winner-take-all rule the only way to change choices is for your current choice to get a lot worse. Formally we can represent winner take all by saying that the probability of visiting a tree is a binary value (0 or 1), based on whether it is the maximum:</p>
	<!-- <div id="katex6-2"></div> -->
	<p>Alternatively you could be choosing probabilistically. That is, how frequently you choose an option will depend on how you value it compared to other options. This rule is called <b>softmax</b>. Softmax sets the probability of going to a tree to be that trees value, divided by the total value available among all trees:</p>
	<div id="katex6-3"></div>
	<p>The advantage of softmax over winner-take-all is that it allows you to continue <b>exploring</b> rather than <b>exploiting</b> the same tree over and over. If it turns out your estimates aren't perfect then exploring is often a better strategy than exploiting. Let's run a simulation to help you understand this idea. Let's call softmax an "explore" strategy and winner-take-all the "exploit" strategy. We're going to have three squirrels below, a red, purple, and blue squirrel. The red and blue squirrels will represent the exploit and explore strategies, respectively (in reality they spend 90% of their time in their preferred mode). The purple squirrel spends half of its time exploiting, and half of its time exploring. Which squirrel do you expect will gain the most apples?</p>
	<p>Whatever your guess is we hope you realized that <i>it depends</i>! If the world is really noisy then you should explore more, if the world is very stable, then you should exploit. You can use the slider bar to control how noisy the world is, the noise is calculated for each visit by randomly adding or subtracting apples to the true value of each tree. The number of apples maxes out at the noise value. Try it out and then make sure you understand the questions at the bottom. Click anywhere in the forest to reset the simulation.</p>
	<input type="range" style="width:200px;" min="1" max="10" step="1" value="3" oninput="learningrate5(this.value);"><span id="noise6">Noise = 3</span><br>
	<canvas id="canvas6" height=500 width=800 onclick="reset6();"></canvas>
	<br><span id="win6">Number of wins. red/exploit: 0 blue/explore: 0 purple/mix: 0</span>
	<div id="plot6"></div>
</div>

<div id="block7">
	<h2>Other sources of information?</h2>
	<p>You've now seen how reinforcement learning can be used to explain your behavior in the task you performed earlier. But what we hid from you in the task was that all the students in the room were doing the task together! What if you could all see each other? How would that additional social information change your behavior? We don't have to imagine this situation--we're just going to try it out live. Connect to <a href="http://gru.stanford.edu:8080" target="_blank">our local forest</a> again and follow your TAs instructions. You'll have to wait for other students to catch up, so this is a good time to ask questions.</p>
</div>

<div id="endblock">
	<h2>Tutorial complete!</h2>
	<p> Your TAs would be happy to discuss any aspect of the tutorial. As a reminder, you should now have a better understanding of the following:</p>
		<ul>
			<li>How reinforcement learning and decision making are used together for action selection</li>
			<li>How the rescorla-wagner and softmax algorithms provide an algorithmic solution for reinforcement learning and decision making</li>
			<li>When these simple algorithms are effective and when they fail</li>
	</ul>
</div>


							</section>
							<section id="continue">
								<a class="button special" onclick="prev();">Go back</a>
								<a class="button special" onclick="next();">Continue</a>
							</section>
						<!-- End Content -->
					</div>
				</div>

			<!-- Footer -->
				<footer id="footer">
					<ul class="copyright">
						<li>&copy; Dan Birman 2015-Present</li><li><a href="https://github.com/dbirman/web/" target="_blank">Code<a/></li>
					</ul>
				</footer>

		</div>

		<!-- Local CSS -->
			<!-- <link rel="stylesheet" href="psych50.css" /> -->
		<!-- Scripts -->
			<script src="../assets/js/shared_code.js"></script>
			<script src="psych50_rl.js"></script>

			<script>launch();</script>
	</body>
</html>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-34324563-2', 'auto');
  ga('send', 'pageview');

</script>
